Metadata-Version: 2.1
Name: sray-db
Version: 3.1.4.0
Summary: SDK to interface with the S-Ray Score database
Home-page: https://github.com/arabesque-sray/score-db
Author: Arabesque S-Ray
Maintainer: StÃ©phan Vuylsteke
Maintainer-email: stephan.vuylsteke@arabesque.com
License: Arabesque Asset Management Ltd. 2019
Description: # sray_db
        
        sray_db is the database abstraction layer library on the highly structured application database of the S-Ray calculation apps. The library should be used by anyone who needs to access or store S-Ray score data.
        
        The objective of sray_db is to facilitate database interactions for S-Ray calculation and loading applications, and to enable the exact calculation of the contribution from one data point to another. To facilitate this, sray_db:
        * abstracts away the underlying database technology: users should only care about which data points they want to store or retreive
        * keeps data fully versioned so point-in-time data can always be stored and retreived with minimal overhead
        * the concept of a `load_cutoff` timestamp offers transparency into which point-in-time data was used as the *input* data for a certain calculation operation
        
        ## Getting Started
        
        ### Prerequisites
        
        To use sray_db as a library, you need to have access to the underlying SQL database. Specifically, you need:
        - server host and port
        - database username and password
        - SSL certificates *(optional)*: root certificate, client key and client certificate. On our GCP PSQL, these files are typically called server-ca.pem, client-key.pem and client-cert.pem.
        
        If you don't have access to the connection details, credentials and/or certificates, please reach out to the GCP administrator.
        
        ### Installing
        
        The package can be downloaded via pip (if you have git installed) using the command:
        `pip install git+https://github.com/arabesque-sray/score-db.git/`
        
        Before importing sray_db, a couple of environment variables (mostly containing secrets) need to be defined. This can be done in any way you prefer. In production environments this is typically done through Kubernetes Secrets. For local development they can be manually added, e.g. using a .env file with IDE plugins or Python packages like python-dotenv.
        
        The environment variables that **need** to be defined are:
        - SRAYDB_SQL_USERNAME
        - SRAYDB_SQL_PASSWORD
        - SRAYDB_SQL_HOST
        - SRAYDB_SQL_PORT
        
        If SRAYDB_SQL_USE_SSL is set to '1', connection to Postgres will be attempted via SSL and as such the following certificates need to be provided:
        - SRAYDB_SQL_SSLROOTCERT *(provide the path to the certificate file)*
        - SRAYDB_SQL_SSLKEY *(provide the path to the certificate file)*
        - SRAYDB_SQL_SSLCERT *(provide the path to the certificate file)*
        
        All the other config variables (see [config.yaml](./sray_db/config.yaml)) **can** be overridden at runtime by simply by providing the similar config key as an environment variable, overriding the value provided in [config.yaml](./sray_db/config.yaml).
        
        ### Usage
        
        sray_db will mostly be used to get and store data into the database. 
        
        #### Get data
        
        Data can be loaded for any field that is described in one of the apps in the AppRegistry (located at sray_db.apps.apps).
        Below example demonstrates one way to load data. Get queries can be customized, see sray_db.query for more detailed examples and use cases.
        
        ```python
        # Import statements
        import pandas as pd
        
        from sray_db.broker import DataBroker
        from sray_db.apps import apps
        from sray_db.apps.pk import PrimaryKey
        from sray_db.query import Get
        
        # load the fields we want to access
        app_name = 'ESG'
        app_version = (2, 6,0, 0)
        field_names = ['esg', 'esg_e', 'esg_s', 'esg_g']
        
        fields = [field for field in apps[app_name][app_version].values() if field.name in field_names]
        
        # Initialize DataBroker
        db = DataBroker()
        
        # Limit data to month-end data for asset 533 (Apple) since 2012
        assets = pd.Index([533], name=PrimaryKey.assetid)
        dates = pd.date_range('2012-01-01', 'now', freq='M', name=PrimaryKey.date)
        
        # We want to load the most recently available data. If instead we'd like to make sure we get data as it was present at a 
        # previous point in time, we can do this by instead declaring as_of as:
        # 
        # as_of = pd.Timestamp('2019-10-05 12:32:39.814247')
        
        as_of = None
        
        # Define query and get data
        get_query = Get(fields, load_idx=[assets, dates], as_of=as_of)
        data = db.query(get_query)
        
        print(f'Loaded {len(data)} results from database')
        ```
        
        #### Store data
        
        Data can be stored for any field that is described in one of the apps in the AppRegistry (located at sray_db.apps.apps).
        To avoid accidental data corruption, the dataset in a Put operation always needs to contain all the fields of an app. 
        When a subset of fields is provided, the operation will fail.
         
        When storing data, 2 considerations need to be made that are described after the example.
        
        ```python
        # Import statements
        import random
        import pandas as pd
        
        from sray_db.broker import DataBroker
        from sray_db.apps import apps
        from sray_db.apps.pk import PrimaryKey
        from sray_db.query import Put
        
        # load the fields we want to store data to
        app_name = 'ESG'
        app_version = (2, 6,0, 0)
        esg_app = apps[app_name][app_version]
        non_system_fields = [field for field in esg_app.values() if not field.is_system_field]
        
        # Initialize DataBroker
        db = DataBroker()
        
        # Create data for Apple on 1900-01-01
        index_names = [PrimaryKey.date, PrimaryKey.assetid]
        data_entry = [[pd.Timestamp('1900-01-01'), 533] + [random.uniform(0, 100) for i in range(0, len(non_system_fields))]]
        data = pd.DataFrame(data_entry, columns=index_names + non_system_fields).set_index(index_names)
        
        # Define load_cutoff and load_idx_to_overwrite
        load_cutoff = None # in this case we haven't done any data loading, so the load_cutoff date is meaningless
        load_idx_to_overwrite = None # we're just updating the data for Apple on a single date, we don't want to invalidate any other existing datapoints
        
        # Define query and store data
        put_query = Put(data, load_cutoff=load_cutoff, load_idx_to_overwrite=load_idx_to_overwrite)
        
        # Confirm we want to affect the database
        if input('Please confirm you want to proceed - this will affect the database you are currently connected to. Type \'YES\' to proceed') == 'YES':
            rows_updated = db.query(put_query)
            print(f'Stored {rows_updated} results into database')
        else:
            print('Aborted')
        ```
        
        ##### 1. load_cutoff date
        To enable a perfect point-in-time contribution analysis, we need to be able to know which exact data points were used as 
        inputs for each calculation. This can be problematic, as data can be updated at any time, including during the 
        calculation runtime. To solve this, we have implemented the `load_cutoff` parameter, representing the maximum 
        `date_created` (i.e. the time a datapoint was published to sray_db) of data that is loaded by the application.
        
        Practically, this means that:
        1. An application should calculate a single `load_cutoff` timestamp before starting any loading, e.g. 
        `load_cutoff = pd.Timestamp.utcnow()`.
        2. In every Get request issued by the application, the `load_cutoff` parameter **has to be provided explicitly** for the 
        `as_of` parameter to ensure no data is loaded with a `date_created` greater than the `load_cutoff` parameter.
        
        ##### 2. load_idx_to_overwrite
        In a versioned database, storing data can mean you're "overwriting" previous records. This requires the user to be 
        thoughtful about how this overwriting should deal with deprecated records, i.e. combinations of PrimaryKeys that were 
        present in the previous insert operation, but not in the current. 
        
        load_idx_to_overwrite is a list of pd.Index items that behaves identical to load_idx in the Get query. If not None, it 
        provides the Put operation information on which existing rows should be 'nullified' if the new Put operation doesn't 
        cover them anymore (i.e. the new dataset does not contain those PrimaryKeys). Any current row that is contained in the 
        load_idx_to_overwrite and not present in the new dataset that is being stored, will automatically be nullified. 
        
        This is best exemplified in a set of three scenarios:
        
        ###### a) Full application re-run
        Applications that are designed to recalculate historical data, will by design store multiple versions of data. In this 
        scenario, the application typically wants to invalidate results that were present in the previous calculation, but not 
        in the current calculation. For example, if we assume that in a first calculation run a result was published for 10 
        companies, but due to different calculation methodology the new calculation run only covers 8 companies. In this case, 
        we probably want to invalidate the data for the 2 missing companies. Simply storing the new data isn't enough, because 
        any future Get requests on the data will still include the 2 missing companies since nothing actually deleted or 
        invalidated that data.
        
        In more simple terms, we want to assure that for each unique PrimaryKey.date, we only keep data from the most recent 
        `date_created`. Practically speaking, this means we first want to load the entire existing dataset, compare it to the 
        new dataset and then set all deprecated rows to null. Since `load_idx_to_overwrite` is passed to the underlying Get 
        query that fetches all existing data, in this scenario we want it to obtain all existing data and as such we leave the 
        list empty:
        ```python
        load_idx_to_overwrite = []
        ```
        
        ###### b) Partial application re-run
        A special case of the previous scenario is when only a subset of data is recalculated. In a hypothetical scenario where 
        an application re-calculates data for three dates (2019-10-01, 2019-11-01, 2019-12-01), we want to assure that all data 
        on those three dates is strictly limited to the new calculation. All other dates should remain unaffected. 
        
        To do so, we can pass the below `load_idx_to_overwrite`:
        
        ```python
        load_idx_to_overwrite = [pd.DatetimeIndex(['2019-10-01', '2019-11-01', '2019-12-01'], name=PrimaryKey.date)]
        ```
        
        ##### c) Fixing bad data points
        In some cases, due to bugs or bad input data one may need to overwrite one or multiple datapoints, without doing a 
        proper re-calculation per se. In this case, we typically don't want to invalidate any datapoints that aren't covered by 
        the new dataset, so we can simply pass a None argument:
        
        ```python
        load_idx_to_overwrite = None
        ```
        
        
        
        #### Delete data
        
        Data can be deleted using a subset of the syntax used for the Get query, i.e. the user can specify the fields for which data to be deleted, as well as an optional list of indexes that limit the deletion to those PrimaryKeys.
        
        To avoid accidental data corruption, the dataset in a Delete operation always needs to contain all the fields of an app. 
        When a subset of fields is provided, the operation will fail.
        
        Since all data in sray_db is point-in-time, no data is actually deleted from the database. Rather, a Delete operation 
        fetches all of the matching datapoints, sets their value to None, and then stores this nullified data using a Put 
        operation. This can make delete operations on large datasets take much longer than an actual SQL DELETE statement.
        
        **Important**: if no load_idx is specified (i.e. an empty list), the DELETE statement is unrestricted and will delete the full history for all assets.
        
        
        ```python
        # Import statements
        import pandas as pd
        from sray_db.broker import DataBroker
        from sray_db.apps import apps
        from sray_db.apps.pk import PrimaryKey
        from sray_db.query import Delete
        
        # load the fields we want to delete
        app_name = 'ESG'
        app_version = (2, 6,0, 0)
        fields = list(apps[app_name][app_version].values())
        
        # Initialize DataBroker
        db = DataBroker()
        
        # Limit deletion to data from 1970-01-01 1970-01-05
        dates = pd.date_range('1970-01-01', '1970-01-05', freq='D', name=PrimaryKey.date)
        assets = pd.Index([533], name=PrimaryKey.assetid)
        
        # Define query and delete data
        delete_query = Delete(fields, load_idx=[assets, dates])
        
        # Confirm we want to affect the database
        if input('Please confirm you want to proceed - this will affect the database you are currently connected to. Type \'YES\' to proceed') == 'YES':
            deleted = db.query(delete_query)
            print(f'Deleted {deleted} results from database')
        else:
            print('Aborted')
        ```
        
        #### Creating new apps
        
        To create a new app, produce the .yaml file that declares all of its attributes. Examples can be found in the [apps](./sray_db/apps/) folder. To add the app to the registry in sray_db.apps.apps, add the path to the .yaml file to the `config_paths` list in [sray_db/apps/\__init\__.py](./sray_db/apps/__init__.py).
        
        In the next paragraph, we show how the corresponding SQL table can be created this newly created app.
        
        #### Manage SQL tables
        
        The database structure is directly defined by the applications, as declared in sray_db.apps.apps. As a result, the tables can be constructed (and deleted) directly from these apps.
        
        ```python
        # Import statements
        from sray_db.apps import apps
        from sray_db.psql.interface import PSQLTable
        
        # 
        app_name = 'SomeAppYouveDesigned' # Make sure not to use an existing / production application when testing
        app_version = [1, 0, 0, 0]  # Make sure not to use an existing / production application when testing
        app = apps[app_name][app_version]
        
        # The interface acts as the layer between an app and its persisted data. Installing an interface creates the underlying SQL table. Similarly, uninstalling the interface drops the SQL table.
        interface = PSQLTable.from_app(app)
        
        # To install the interface (and create the table), simply run
        if input('Please confirm you want to proceed - this will try to create a new table in the database you are currently connected to. Type \'YES\' to proceed') == 'YES':
            interface.install()
            print('Interface installed.')
        
        # To uninstall the interface (and drop the table), simply run
        if input('Please confirm you want to proceed - this will try to drop a table in the database you are currently connected to. Type \'YES\' to proceed') == 'YES':
            interface.uninstall()
            print('Interface uninstalled.')
        
        ```
Platform: UNKNOWN
Requires-Python: >=3.7
Description-Content-Type: text/markdown
